<head>
  <script src="//cdnjs.cloudflare.com/ajax/libs/dygraph/2.0.0/dygraph.min.js"></script>
  <link rel="stylesheet" src="//cdnjs.cloudflare.com/ajax/libs/dygraph/2.0.0/dygraph.min.css" />
</head>
<body>
<div style="height:20px;"></div>
<div style="width:60%;margin:auto;">
<h1> [documentation] <a href="https://github.com/SolbiatiAlessandro/RNN-stocks-prediction">RNN-stocks-prediction</a></h1>
<h3> Another attempt to use Deep-Learning in financial markets*</h3>
<p><b>project mission</b>: implement some AI systems described in research papers in a full-stack application deployed to the market.</p>
  
<p style="font-size:9px">* this web page is left without proper formatting on purpose </p>
<div>

<div style="height:20px;"></div>

<div style="width:90%;margin:auto;">
<p>Recently I have been playing around with AI and Deep-Learning and I came out with this idea about DL applied to financial markets. Turns out that there are PLENTY of people already doing that out-there, here is a non-comprehensive list:</p>
<ul>
  <li><a href="https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02">'deep-learning-the-stock-market', Tal Perry</a></li>
  <li><a href="http://www.jakob-aungiers.com/articles/a/LSTM-Neural-Network-for-Time-Series-Prediction">'LSTM-Neural-Network-for-Time-Series-Prediction', Aungiers</a></li>
  <li>'Recurrent neural networks approach to the financial forecast of Google assets', Di Persio, Honchar, 2017</li>
  <li>'Artificial neural networks apporach to the forecast of stock market price movements', Di Persio, Honchar, 2016</li>
</ul>
<p> After an intensive summer-school about DL and intensive reading of papers about the topic I decided to start my own project: here we are.</p>
</div>

<div style="height:20px;"></div>
<div style="width:90%;margin:auto;">
  <h2>Day 1 - kickstarting with regression</h2>
  <img src="https://raw.githubusercontent.com/SolbiatiAlessandro/RNN-stocks-prediction/master/docs/img/day1.jpg" width="500px" />
  <p>This is my work-desk on Day1. So after extensive reading of already existing implementation on the web about RNN to fin markets I am bit disappointed. Most of the time people have no idea what they are speaking about either because they don't know anything about finance or because they don't know anything about the foundation of Deep Learning (or both). On the other hand, what excites me most is that there are plenty of papers about people claiming to predict stock prices with accuracy from 65% to 80%, that is impressive. So my guideline is to try to implement those RNN models in my application, and apply the prediction to some financial instruments. Currently I would go for binary options, it seems fitting pretty well with the RNN prediction capabilities. There are two main types of approach to the problem:<p>
  <ul>
    <li><b>(A) Regression problem</b>: the most straightforward, given the last Ndays predict tomorrow closing price.</li>
    <li><b>(B) Classification problem</b>: given the last Ndays, will the price tomorrow goes up or down? (binary classification).</li>
  </ul>
  <p>Full of example of (A) on the internet, and the truth it that they don't work so well. Anyway since they are relatively easy to impelement I gave it a try and here is the outcome.</p>
  <div id="graphdiv3" style="width:500px; height:300px;"></div>
  <p style="font-size:12px">This is graph is the normalized closing price of GOOGL(on a 30periods window), with actual and predicted. I trained the NN with 4 years of prices and 10 epochs.</h8>
  <p>So the graph is showing 'acutal' as the target price and 'predicted' as the price that the NN predicts. How does it work? Is really simple.</p>
  <ul>
    <li>use Pandas to download and create a DataFrame with OHLCV data</li>
    <li>create the tensor of dimensions (Batch_Size, 30, 5), so 30 time steps and 5 features</li>
    <li>split train and test data</li>
    <li>build the RNN model with Keras: I used LSTM + Dropout, a really simple implementation</li>
    <li>train the model,used 100 batch_size and 10 epochs</li>
    <li>predict the outcome from X_Test and compare the prediction with actual values</li>
  <p>The full code of this implementation is in the repo <a href="https://raw.githubusercontent.com/SolbiatiAlessandro/RNN-stocks-prediction/master/regression.py">regression.py</a></p>
  <p>So the predictions are really bad, why is that? Of course the NN had not enough training data and the model is quite too simple: the papers suggest to implement at least two stacked LSTM layers with dropout and everything. Regarding the data (now the NN is trained only on GOOGL prices from 2012 to 2016) I should give the NN at least 20/30years of stock prices to acquire some consistency in the predictions. Anyway, this first example was interesting as shows that the NN is actually working an able to detect the trend in stock prices, even if not ready to do any useful prediction right now.</p>
  <div id="graphdiv4" style="width:500px; height:300px;"></div>
  <p style="font-size:12px">Same as above but this time the NN is trained with 16 years of stock prices, what is happening here is that the NN just take yesterday price and change a bit the value with no big difference. Thisresult is actually really bad from a prediction point of view, but is often misjudged as 'excellent' by people using Keras for the first time thinking that the NN is predicting the future: well, is not. Is just copying yesterday price.</h8>
  <p> The problem with regression is that the NN is not actually learning any pattern in the market, is just saying 'tomorrow price is the same as yesterday'. I found literally dozens of online courses and similar where they don't really understand what's going on and think that the NN is actually predicting with 100% accuracy the price of tomorrow, last graph below for regression problem, I trained the NN with 20years of data using a more complex model but the outcome doesn't change.</p>
  <div id="graphdiv5"style="width:500px; height:300px;"></div> 
  <p><b>Regression or Classification?</b> I would stop here improving the regression model (A) and spend my energy for the classification model (B). Why is that? In my opinion regression model is not the answer, as the OHLCV data are not enough for this kind of prediction. If we really want to predict the stock price consistently we need some sentiment analysis and other interesting things in our model, furthermore there is the accuracy question: if I am predicting the price I need a really high accuracy to implement it in a trading system or something. On the otherside, the classification problem woulde be extremely useful even if solved with a 60-70% accuracy. Why is that? I am thinking about some trading strategy with binary options. For now suffice it to say that if I am really able to solve the classification problem with something like 68% accuracy I am done: binary options have a break even of 53% and implementing an algo trading on that shouldn't be so difficult. Let's see what's next</p> 
</div>
<div style="height:20px;"></div>
<div style="width:90%;margin:auto;">
  <h2>Day 2 - classification time</h2>
  <img src="https://raw.githubusercontent.com/SolbiatiAlessandro/RNN-stocks-prediction/master/docs/img/day2.jpg" width="500px" />
  <p>BIGNEWS: I managed to speak with a PhD in DeepLearning from Tsinghua-Uni and we will have lunch sat, hope he will gives me great insight on what I am doing right/wrong.</p>
  <p>As I said, today I will focus on classification problem. So how does it work? This is my view up to now.</p>
  <ul>
    <li> INPUT - 30-days window <b>normalized log-returns</b>: a log return is defined with a function like this 'ret = lambda x,y: log(y/x)', and the returns must be normalized with a funct like this 'zscore = lambda x:(x -x.mean())/x.std()'. Easy stuff. The not-so-easy stuff here is whether to consider all the returns on OHLCV <b>(option I2)</b> or only the returns on Close prices <b>(option I1)</b>. I will consider and try both options: I2 means considering much more info (5 features in the NN) about the financial asset that could not be so relevant, instead I1 is about just using closing prices (1 feature implementation) that should be more straightforward <i>for humans</i>. I will try both options and see how the NN react. This time we will have an accuracy value to tell us some detailed info about the performance of our dear NN.</li>
    <li> OUTPUT - <b>label</b> for tomorrow behaviour of (closing)price: also here we have a couple of choice to make, the first choice <b>(option O1)</b> is about a binary classification in the label, i.e. a binary vector [1,0] or [0,1] to show if the price goes up or down. The second choice <b>(option O2)</b> is implementing a multi-class classification, that could be understood as a "discrete mapping" from the continuous log return interval [-INF; +INF] to a discrete interval {-5, -4, .. , 4, 5}. As before, O2 is more difficult to implement and carry out more information, I will have a try with both and see the different outcomes.</li>
  </ul>
  <p>After a few hours of testing and reading some papers about this topic I figured out that really good accuracy is obtained with the tuple I2-O1, that is multivariate input of all the OLHCV data and as output a binary classification [1,0] or [0,1] (not multi-class) of the predicted behaviour of the price. I will go for that one, as my next framework implementation.</p>
  <p><b>- ATTENTION: IMMINENT BREAKTHROUGH -</b></p>
  <p>So I wrote the Neural Network with I2-O1 configuration and the results were suprisingly good, on the training data of GOOGL stock price from 2000-2016 the NN got a <u>prediction accuracy of 63%</u>. What does it mean? That if you gave the last 30 days OHLCV prices from Yahoo Finance to the Neural Network it will be predict the price of tomorrow with an accuracy of 63% (right 63 times on 100). This is a great news since it mean that the NN is actually working and acquiring some prediction capabilities. Still, the validation accuracy is between 52-54%, so there is a large room for improvement. Here is the training accuracy and validation accuracy for the model.</p>
  <div id="graphdiv6"style="width:500px; height:300px;"></div> 
  <p>Right now I am pretty positive, since the project is becoming meaningful. I am actually creating something that is able to predict prices consistently and <b>improves</b> during the training. This concept of 'training' the Neural Network really excites me, I mean seeing actually the NN getting better and better at predicting stock prices during the training. To highlight this concept I show you here the 'loss function', that is the difference between the predicted and actual value. As you see, during the training the loss is decreasing more and more (wow!). Of course, the model is not perfect and is still overfitting (as you can see is getting better and better on training data while worse on new validation data), how to avoid overfitting? Increase dropout, reduce complexity.. mmm actually the NN just needs more data. We will try to do that afterward.</p>
  <div id="graphdiv7"style="width:500px; height:300px;"></div> 
  <p><b>PLOT TWIST: I DID NOT USE RNN</b></p>
  <p>Yeah I didn't use a recurrent neural network model, no LSTM, no GRU, nothing. I used.. CNN! I found on this paper [Di Persio, Honchar 2017] the description of this Convolutional Neural Network model suitable for my O1-I2 model and it just worked perfectly. I will go thourgh the model implementation later, now I want to do a better training to actually see what this NN can do. So the next steps should be something like this:</p>
  <ul>
    <li>Prepare a <b>massive training</b>, using a full-index data like S&P500, I need to write some function to iterate over different tickers training the same model. This way I hope to get a first enhancement in the accuracy</li>
    <li>Analyise the CNN model, try to do some fine-tuning changing hyper-parameters and other features</li>
    <li>Implement the same structure with a RNN model, it should technically perform better</li>
    <li>If I get a decent accuracy (60%) also on the validation sample I can start to do some algo-trading benchmarking to evaluate the next steps</li>
  </ul>
  <p>After this long day we got some result: EUREKA. Now will start the long journey to improve the accuracy as high as possible. I will probably spend the next hours playing around with the classification model on different financial instruments to see if I get some golden nugget (FOREX, Volatility and others)</p>
</div>
<div style="height:20px;"></div>
<div style="width:90%;margin:auto;">
  <h2>Day 3 - A deep unexpected model</h2>
  <img src="https://github.com/SolbiatiAlessandro/RNN-stocks-prediction/blob/master/docs/img/day3.jpg?raw=true" width="500px" />
  <p>I am starting my third day of work on this project quite positive, after the GOOGL training set yesterday I tried to train the CNN on larger dataset and I got even better result. But <b>let's go through the CNN Model to understand how it work.</b></p>
  <ul>
    <li><b>INPUT Dataframe structure</b>: the input data of the model is a tensor of shape (BATCH_SIZE, 30, 5), BATCH_SIZE is the length of our training data, 30 (Days) is the window size of prices that are 'looked backward' and 5 is the number of features (Open,High,Low,Close,Volume) [see my choice of I2 above]. Since we are choosing windows of 30days we are normalizing the 5 features with a zscore function on these 30 days.</li>
    <li><b>OUTPUT Dataframe structure</b>: the output data model is a binary vector [1,0] or [0,1] [see my choice O1 above]. This is simply a vector with two components [p1, p2], where p1 is the probability of the price moving down and p2 is the probability of the price moving up. (yes, always p1+p2=1)</li>
    <li><b>Data splitting (train/test)</b>: after the dataframe is created the data are splitted between training part and testing part with 9:1 proportion. Currently the dataframe (with different stocks) is simply split with 9:1 proportion on the whole data set, not stock by stock.</li>
    <li><b>Model (part 1)</b>: the first part of the CNN model is composed by two hidden convolutional layers (Conv1D) that take the INPUT Dataframe and identify the 'patterns' that cause the price movements (protip: check Convolution on Wikipedia)</li>
    <li><b>Model (part 2)</b>: the second part of the CNN is two dense layers + softmax activation that 'reduce' the data to a binary vector [p1, p2]
    <li><b>Prediction</b>: so after the model is trained for every input (30, 5) we get an output [p1, p2]. If p1>p2 the prediction is that price will go down, p2>p1 the price will go up.</li>
  </ul>
  <p>The model right now is still quite simple and there are dozens of different implementations and variations that can be tried. My view up to now is that the key point is <i>find the right balance with the kind of data to predict and the right complexity of the Neural Network</i>. Here below is a scheme about the structure of the implementation up to now.</p>
  <img src="https://github.com/SolbiatiAlessandro/RNN-stocks-prediction/blob/master/docs/img/day3b.jpg?raw=true" width="500px" />
  <p>Ok so we have this model, to see how it actually perform with a large scale dataset <b>I tried to run it on the SP500</b>. Since I don't have any supercomputer around I just tried with my MacBook GPU on the first 15 titles of the SP500, namely ['ABT', 'ABBV', 'ACN', 'ADBE', 'AAP', 'AES', 'AET', 'AFL', 'AMG', 'A', 'GAS', 'APD', 'ARG'] -> about 40000 entries in the dataframe. I got different results from the previous GOOGL training set (4000 entries), let's have a look.</p>
  <div id="graphdiv8" style="width:500px; height:300px;"></div>
  <p style="font-size:12px">training dataset 15 titles from SP500, 110 epochs with a BATCH_SIZE of about 40000 entries, the final effective accuracy is about 54-55%</p>
  <div id="graphdiv9" style="width:500px; height:300px;"></div>
  <p style="font-size:12px">training dataset 15 titles from SP500, 110 epochs with a BATCH_SIZE of about 40000 entries, this time the loss is decreasing, not so much but is decreasing at least. So we DO NOT have overfitting </p>
  <p>So having a look in hindsight about the performance of the NN on the GOOGL training data, the main problem is <i>overfitting</i>. In that case the loss function for the training dataset is decreasing while the loss function for validation data is increasing. This means that the NN while is learning really good the distribution of GOOGL prices but not really improving its prediction ability (about 52% for GOOGL validation data). <b>Now on the SP500 dataset we have a substantial improvement on this</b>. You can see that the loss value is actually decreasing BOTH for the training and validation: the predictivity of the NN is improving and the final result is in fact 54-55% predictivity on the actual validation set. Yes, now we are going towards underfitting and this is why we will need to take a look at our architecture and start to tune it to get better performance, but.. hey, now is really working. We are predicting on 55% accuracy new data. You can have a try to predict your own data loading my NN that you can find in .hfd5 format in the github repo <a href="https://github.com/SolbiatiAlessandro/RNN-stocks-prediction/blob/master/SP500classification_model.hdf5">here</a>.</p>.
  <p>Note: how did I extend the training from one stock to a full index? I wrote a small framework that basically iterate the data preprocessing I used with google with a larger dataframe that operate with different tickers, probably this framework will be the foundation of the full-stack application</p>
  <p><b>What's next now?</b> according to what I wrote yesterday and how my perspective is evolving I see two ways I can procede from now, both equally promising.</p>
  <ul>
    <li><b>The way of the <i>BALANCE</i></b>: the way to improve prediction capability is to find the right balance between the complexity of the data (using different timeframes, or adding more indicators and features) and the complexity of the architecture (how many layers, activation and error function). To quote the dear J. Bengio, 'tuning a NN is an art': it requires a good expertise in the field, so I consider quite challenging and exciting as a choice. Probably will lead to the best results.</li>
    <li><b>The way of the <i>CHANGE</i></b>: the other way is to change the CNN architecture to a different one, based on my readings I would say that other promising DNN arch. are RNN(LSTM), RNN(GRU), RNN(LSTM+Wavelet), RNN(GRU+Wavelet) and RNN(ELSM). Changing an architecture just means use a different implementation in the model, and thank to the AMAZING <a href="https://github.com/fchollet">Francois</a> that created this amazing tool called Keras, is pretty easy to implement different architecture. Hence, I would consider this way as the most straightforward but could reveals other breakthrough during the way. </li>
  </ul>
  <p><b>- ATTENTION: IMMINENT BREAKTHROUGH -</b></p>
  <p>So I started with <i>'the way of the Balance'</i>, because I wanted to have a better understanding of the data I am working on: I started playing around with different timeframes and different combination of inputs and after some trials I got an <b>impressive improvement</b> in the performance of my CNN. We all know how scientific progress is usually all about luck, and even in this case of course I had some luck in finding this pattern just after a couple of hours; but as a personal disclosure I would say that I had a kind of intuition on that. Let me briefly go through my mental process.</p>
  <p><b>INTUITION</b>: so the CNN is not working SO good on 1 day forecast, and this is due to a high noise and random motion in stock prices on a short term. The CNN should be good at individuating some 'patterns' that regulate stocks prices movement and to improve the performance I should try to find a situation where this pattern are more evident without all that noise from daily prices and Random/Brownian motion. Let's see, what if I increase the time frame? If I tell my CNN not to predict tomorrow prices but to predict next week price I should be able to take away most of the noise and actually spot those valuable patterns I am looking for. <b>Long-story-short it works, the CNN improves from a 53% to a 57% prediction accuracy on new data.</b> Let's see the CNN in action during the training.</p>
  <img src="https://github.com/SolbiatiAlessandro/RNN-stocks-prediction/blob/master/docs/img/day3c.png?raw=true" width="900px" />
  <p style="font-size:12px">Accuracy (up) and Loss (down) of the CNN doing binary classification on stock movements with 1 day predictions (left) and 5 day predictions (right). [The CNN was again trained on 15 titles of the SP500 (about 40000 entries) with 100 epochs]</p>
  <p>So here we have on the left the CNN predicting stocks for 'tomorrow' (1 day prediction), and on the right the CNN predicting stock for 'next week' (5 day prediction). So from the top two charts we see an <b>impressive improvement in the prediction accuracy on new data (validation sample) going from 53%-54% on 1-day forecast to 56-57% on 5-day forecast.</b> Why I am so excited by this result? Because the CNN is now actually able to make <u>useful predictions</u>: the 'validation accuracy' on new data is infact the real accuracy that the Neural Network has when is predicting stocks on the market. To validate this I gave the CNN the last ten years prices of a random title on SP500 and she actually predicted the stock movement correctly 57% of the times (4000 samples, so this 57% is statistically consistent and not just a random output). Amazing, I can feel the scientific progress taking action here. So let's try to analyse this chart and making some objective statments on our CNN, I will call (A) the 1-day forecast and (B) the 5-days forecast.</p>
  <ul>
    <li>1 - We can observe that in (B) there is an increase in the noise of the data, specifically from the loss function on validation sample.</li>  
    <li>2 - There is an increase in (B) both in the training accuracy and in the validation accuracy, but interestingly enough in (A) <i>Train Acc > Val acc</i> and in (B) <i>Val acc > Train Acc</i>. </li>
    <li>3 - (A) shows a lower but increasing validation accuracy and (B) shows a higher but not-increasing validation accuracy. This is correlated with the different loss values (crossentropy) on validation sample: higher and decreasing loss in (A), lower, not-so-decreasing and with a high noise loss in (B). (I made some linear regression on the data to check this point)</li> 
    <li>4 - Speaking about training values, (A) and (B) are similar regarding slope and noise but (B) is better at learning the distribution of the data given higher value of train acc and lower value of train loss.</li>
  </ul>
 <div style="height:20px;"></div>
  
<script type="text/javascript">
  g3 = new Dygraph(
    document.getElementById("graphdiv3"),
    "https://raw.githubusercontent.com/SolbiatiAlessandro/RNN-stocks-prediction/master/docs/csv/regressionA.csv",
    {
      rollPeriod: 1,
      showRoller: true,
      ylabel: 'stock price (normalized)',
      xlabel: 'time (days)'
    }
  );
  g4 = new Dygraph(
    document.getElementById("graphdiv4"),
    "https://raw.githubusercontent.com/SolbiatiAlessandro/RNN-stocks-prediction/master/docs/csv/regressionB.csv",
    {
      dateWindow: [80,120],
      rollPeriod: 1,
      showRoller: true,
      ylabel: 'stock price (normalized)',
      xlabel: 'time (days)'
    }
  );
    g5 = new Dygraph(
    document.getElementById("graphdiv5"),
    "https://raw.githubusercontent.com/SolbiatiAlessandro/RNN-stocks-prediction/master/docs/csv/regressionC.csv",
    {
      dateWindow: [80,120],
      rollPeriod: 1,
      showRoller: true,
      ylabel: 'stock price (normalized)',
      xlabel: 'time (days)'
    }
  );
   g6 = new Dygraph(
    document.getElementById("graphdiv6"),
    "https://raw.githubusercontent.com/SolbiatiAlessandro/RNN-stocks-prediction/master/docs/csv/classification_acc.csv",
    {
      rollPeriod: 1,
      showRoller: true,
      ylabel: 'accuracy (%)',
      xlabel: 'training time (epochs)'
    }
  );
   g7 = new Dygraph(
    document.getElementById("graphdiv7"),
    "https://raw.githubusercontent.com/SolbiatiAlessandro/RNN-stocks-prediction/master/docs/csv/classification_loss.csv",
    {
      rollPeriod: 1,
      showRoller: true,
      ylabel: 'loss (%)',
      xlabel: 'training time (epochs)'
    }
  );
     g8 = new Dygraph(
    document.getElementById("graphdiv8"),
    "https://raw.githubusercontent.com/SolbiatiAlessandro/RNN-stocks-prediction/master/docs/csv/SP500classification_acc.csv",
    {
      rollPeriod: 1,
      showRoller: true,
      ylabel: 'accuracy (%)',
      xlabel: 'training time (epochs)'
    }
  );
     g9 = new Dygraph(
    document.getElementById("graphdiv9"),
    "https://raw.githubusercontent.com/SolbiatiAlessandro/RNN-stocks-prediction/master/docs/csv/SP500classification_loss.csv",
    {
      rollPeriod: 1,
      showRoller: true,
      ylabel: 'loss (%)',
      xlabel: 'training time (epochs)'
    }
  );
  
</script>
</body>

